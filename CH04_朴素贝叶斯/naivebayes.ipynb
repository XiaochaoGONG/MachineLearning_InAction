{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于概率论的贝叶斯理论，计算给定条件下，分类的概率。通过比较概率的大小判断分类。  \n",
    "之所以称为‘**朴素**’是因为需要  \n",
    "假设1各个条件之间是相互独立的，联合概率就等于各个概率的乘积。  \n",
    "假设2各个特征同等重要。  \n",
    "优缺点：  \n",
    "* 优点：数据较小情况下也能处理；可处理多类别问题\n",
    "* 缺点：输入数据要求敏感\n",
    "适用于标称数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 使用条件概率分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件概率公式：  \n",
    "$$p(c_i|w) = \\frac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "如果$p(c_1) > p(c_2)$，那么属于类别1，否则类别2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 示例：文档分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档分类中，整个文档是实例，而文档中某些元素是特征。可以将每个常用词的出现或者不出现作为一个特征，这样尽管每个实例都长度不同，特征数目都是一样多的。  \n",
    "分类标签区别侮辱性和非侮辱性，使用1和0表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 准备数据\n",
    "我们将训练的文本进行转换，将一句话划分成词汇集合，再将特征词汇出现与否进行矩阵转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadDataSet():\n",
    "    '''\n",
    "    创建一个示例作为数据集\n",
    "    Return: 特征单词矩阵; 分类标签向量\n",
    "    '''\n",
    "    import re\n",
    "    postingSentense = ['My dog has flea problems, help please.', 'Maybe not take him to dog park stupid',\n",
    "                      'My dalmation is so cute, I love him', 'Stop posting stupid worthless garbage',\n",
    "                      'Mr Licks ate my steak, how to stop him', 'Quit buying worthless dog food, stupid.']\n",
    "    \n",
    "    ClassVec = [0, 1, 0, 1, 0, 1]\n",
    "    PostingList = []\n",
    "    for sentense in postingSentense:\n",
    "        postSentense = re.sub('[,\\.]', '', sentense)\n",
    "        \n",
    "        PostingList.append(postSentense.lower().split(' '))\n",
    "        \n",
    "    return PostingList, ClassVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'i', 'love', 'him'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LoadDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateVocabList(DataSet):\n",
    "    VocabSet = set([])\n",
    "    for data in DataSet:\n",
    "        VocabSet = VocabSet | set(data)\n",
    "    return list(VocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SetOfWords2Vec(VocabList, InputSet):\n",
    "    '''\n",
    "    判断输入的句子中是否含有词汇表中的特征词汇\n",
    "    Return: 是否含有特征词汇的矩阵\n",
    "    '''\n",
    "    Vec = [0] * len(VocabList)\n",
    "    for word in InputSet:\n",
    "        if word in VocabList:\n",
    "            Vec[VocabList.index(word)] = 1\n",
    "        else:\n",
    "            print 'word %s is not in the vocab list.' % word\n",
    "        \n",
    "    return Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PostingList, Class = LoadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VocabSet = CreateVocabList(PostingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print SetOfWords2Vec(VocabSet, PostingList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print SetOfWords2Vec(VocabSet, PostingList[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 训练算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面过程我们得到了一个矩阵，下面我们将根据条件概率的公式，进行判断句子是否属于侮辱性。  \n",
    "由于每个单词出现的概率在假设中是固定的，所以我们比较的时候将$p(w)$略去。  \n",
    "那么比较$p(c_i|w)$也就是比较$p(w|c_i)p(c_i)$  \n",
    "其中$p(c_i)$是整个样本中出现侮辱性的概率，$p(w|c_i)$是在那些侮辱性样本中，出现特征词汇的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainNB0(D, Y):\n",
    "    m = len(D)\n",
    "    numWords = len(D[0])\n",
    "    PC = sum(Y) / float(m)    # p(c)\n",
    "    # 侮辱性的统计\n",
    "    p1Num = np.ones(numWords)\n",
    "    p1Denom = 2.0\n",
    "    # 非侮辱性的统计\n",
    "    p0Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        if Y[i] == 1:\n",
    "            p1Num += D[i]\n",
    "            p1Denom += sum(D[i])\n",
    "        else:\n",
    "            p0Num += D[i]\n",
    "            p0Denom += sum(D[i])\n",
    "    P1Vec = np.log(p1Num / p1Denom)\n",
    "    P0Vec = np.log(p0Num / p0Denom)\n",
    "    return P1Vec, P0Vec, PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = []\n",
    "for data in PostingList:\n",
    "    D.append(SetOfWords2Vec(VocabSet, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P1, P0, PC = TrainNB0(D, Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到两个概率后，我们可以计算条件概率，$P1*PC$和$P0*(1-PC)$  \n",
    "由于P1中各个概率做乘法只要有一个为0就会使整个式子为0，所以将初始化修改为1，并将分母修改为2  \n",
    "对于计算乘法，由于大部分因子较小，会导致最终的结果精度溢出，得到0，所以修改乘法为加法，在上面的函数中还没有涉及到乘法的运算，但是需要提前处理数据，将乘法转换成加法的一个方法就是两边取自然对数，这样不会影响结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testD1 = ['love', 'my', 'dalmation']\n",
    "testD2 = ['stupid', 'garbage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将语句转换成矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1 = SetOfWords2Vec(VocabSet, testD1)\n",
    "D2 = SetOfWords2Vec(VocabSet, testD2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算条件概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([-3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -2.35137526, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -2.35137526, -2.35137526, -3.04452244, -1.94591015, -3.04452244,\n",
       "        -2.35137526, -3.04452244, -2.35137526, -3.04452244, -1.94591015,\n",
       "        -3.04452244, -1.65822808, -3.04452244, -2.35137526, -3.04452244,\n",
       "        -3.04452244, -3.04452244]),\n",
       " array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "        -3.25809654, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.15948425,\n",
       "        -3.25809654, -3.25809654, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "        -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -1.87180218]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(D1), np.array(D2), P1, P0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据公式$$p(c_i|w) = \\frac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "我们计算时去掉$p(w)$，并且计算$p(w_1|c_i)p(w_2|w_i)$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PC1D1 = np.sum(np.multiply(D1, P1)) + np.log(PC)\n",
    "PC0D1 = np.sum(np.multiply(D1, P0)) + np.log(1.0 - PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if PC1D1 > PC0D1:\n",
    "    print 1\n",
    "else:\n",
    "    print 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PC1D2 = np.sum(np.multiply(D2, P1)) + np.log(PC)\n",
    "PC0D2 = np.sum(np.multiply(D2, P0)) + np.log(1.0 - PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if PC1D2 > PC0D2:\n",
    "    print 1\n",
    "else:\n",
    "    print 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合成函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ClassifyNB(D, P1, P0, PC):\n",
    "    pc1D = np.sum(np.multiply(D, P1)) + np.log(PC)\n",
    "    pc0D = np.sum(np.multiply(D, P0)) + np.log(1.0 - PC)\n",
    "    if pc1D > pc0D:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClassifyNB(D1, P1, P0, PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClassifyNB(D2, P1, P0, PC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 示例：垃圾邮件过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 词袋模型\n",
    "将每个词都出现与否作为一个特征，称为**词集模型(set-of-words)**，而如果一个词出现不止一次，并且也要把这个作为特征部分，就需要另一种统计，称为**词袋模型(bag-of-words)**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BagOfWords2Vec(VocabList, InputSet):\n",
    "    Vec = [0] * len(VocabList)\n",
    "    for word in InputSet:\n",
    "        if word in VocabList:\n",
    "            Vec[VocabList.index(word)] += 1\n",
    "    return Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 准备数据\n",
    "处理电子邮件，首先要将字符串抽象成矩阵数字，采用上面类似的方法，不同的是在转换方法上有些区别，考虑更多种情况。  \n",
    "如果直接使用string.split()切分会有标点符号存在，所以采用正则表达式re来处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mySent = 'This book is the best book Python or M.L. I have ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TextParse(S):\n",
    "    import re\n",
    "    listOfwords = re.split(r'\\W*', S)\n",
    "    return [word.lower() for word in listOfwords if len(word) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'python',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextParse(mySent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将文件夹中的邮件导入，并解析为词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetMailToVec():\n",
    "    '''\n",
    "    返回测试集和训练集矩阵\n",
    "    '''\n",
    "    import os\n",
    "    spamPath = os.getcwd() + '/email/spam/'\n",
    "    hamPath = os.getcwd() + '/email/ham/'\n",
    "    docList = []\n",
    "    classList = []\n",
    "    for f in os.listdir(spamPath):\n",
    "        wordList = TextParse(open(spamPath + f).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = TextParse(open(hamPath + f).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(0)\n",
    "        \n",
    "    vocabList = CreateVocabList(docList)\n",
    "    \n",
    "    # 划分\n",
    "    alpha = 0.8\n",
    "    totalNum = len(os.listdir(spamPath)) + len(os.listdir(spamPath))\n",
    "    totalSet = range(totalNum)\n",
    "    np.random.shuffle(totalSet)\n",
    "    trainingSet = totalSet[:int(totalNum * alpha)]\n",
    "    testSet = totalSet[int(totalNum * alpha):]\n",
    "    \n",
    "    TrainMat = []\n",
    "    TrainClasses = []\n",
    "    TestMat = []\n",
    "    TestClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        TrainMat.append(SetOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        TrainClasses.append(classList[docIndex])\n",
    "    for docIndex in testSet:\n",
    "        TestMat.append(SetOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        TestClasses.append(classList[docIndex])\n",
    "    \n",
    "    return TrainMat, TrainClasses, TestMat, TestClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrainD, TrainY, TestD, TestY = GetMailToVec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P1, P0, PC = TrainNB0(TrainD, TrainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err = 0\n",
    "for i in range(len(TestD)):\n",
    "    pred = ClassifyNB(TestD[i], P1, P0, PC)\n",
    "    if pred != TestY[i]:\n",
    "        err += 1.0\n",
    "rate = err / len(TestD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
